{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra\n",
    "\n",
    "Attempt at streamlining the whole code, as is quite messy now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apogee.tools.read as apread\n",
    "import apogee.tools.path as apogee_path\n",
    "from apogee.tools import bitmask\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.nn as nn\n",
    "import os\n",
    "\n",
    "from torchcontrib.optim import SWA\n",
    "\n",
    "from apoNN.src.datasets import ApogeeDataset\n",
    "from apoNN.src.utils import dump as dump \n",
    "from apoNN.src.utils import load as load \n",
    "from apoNN.src.vectors import *\n",
    "from tagging.src.networks import ConditioningAutoencoder,Embedding_Decoder,Feedforward,ParallelDecoder,Autoencoder\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "apogee_path.change_dr(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data=6000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "allStar= apread.allStar(rmcommissioning=True,main=False,ak=True, akvers='targ',adddist=False)\n",
    "\n",
    "upper_temp_cut = allStar[\"Teff\"]<7000\n",
    "lower_temp_cut = allStar[\"Teff\"]>3500\n",
    "lower_g_cut = allStar[\"logg\"]>1.\n",
    "upper_g_cut = allStar[\"logg\"]<3.5\n",
    "snr_cut = allStar[\"SNR\"]>100\n",
    "snr_highcut = allStar[\"SNR\"]<500\n",
    "\"\"\"feh_outliercut = allStar[\"Fe_H\"]>-5\n",
    "o_outliercut = allStar[\"O_FE\"]>-5\n",
    "c_outliercut = allStar[\"C_FE\"]>-5\n",
    "na_outliercut = allStar[\"Na_FE\"]>-5\n",
    "mg_outliercut = allStar[\"Mg_FE\"]>-5\n",
    "si_outliercut = allStar[\"Si_FE\"]>-5\n",
    "al_outliercut = allStar[\"Al_FE\"]>-5\n",
    "s_outliercut = allStar[\"S_FE\"]>-5\n",
    "p_outliercut = allStar[\"P_FE\"]>-5\n",
    "ti_outliercut = allStar[\"Ti_FE\"]>-5\n",
    "cr_outliercut = allStar[\"Cr_FE\"]>-5\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "combined_cut = lower_g_cut & upper_g_cut & lower_temp_cut & upper_temp_cut & snr_cut & snr_highcut & feh_outliercut & o_outliercut &  c_outliercut & na_outliercut & mg_outliercut & si_outliercut & al_outliercut & p_outliercut & s_outliercut & ti_outliercut & cr_outliercut\n",
    "cut_allStar = allStar[combined_cut]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickled_dataset = ApogeeDataset(filename=\"aspcap_extended\",outputs = [\"aspcap\",\"mask2\",\"physical\",\"idx\"])\n",
    "autoencoder = torch.load(\"/share/splinter/ddm/taggingProject/apogeeFactory/outputs/guild/z10/ae_3600.p\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = LatentVector(pickled_dataset,autoencoder,n_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "considered_parameters = [\"Teff\",\"logg\",\"Fe_H\",\"O_FE\",\"C_FE\",\"Na_FE\",\"Mg_FE\",\"Si_FE\",\"S_FE\",\"Al_FE\",\"P_FE\",\"Ni_FE\"] \n",
    "y = Vector(np.array([cut_allStar[param] for param in considered_parameters])[:,:n_data].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear algebra\n",
    "\n",
    "With the linear algebra. Training a new model becomes incredibly easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = LinearTransformation(z,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = w.predict(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 9\n",
    "plt.scatter(y.raw[:,i],y_pred.raw[:,i],s=0.5)\n",
    "plt.title(considered_parameters[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y.raw[:,0],y.raw[:,1],s=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also possible to view the outputs for any directions. Not just those picked by the linear transformation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y.raw[:,0],np.dot(w.val[0],z.centered.T),s=0.5)\n",
    "#plt.scatter(y.raw[:,2],np.dot(test_pca,z.centered.T),s=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the PCA breakdown\n",
    "\n",
    "We can fit PCA to the latent space to find the directions of the hyperplane containing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=10,whiten=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(z.centered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see that not every single dimension contains as much variations. One trick is then we can transform the dataspace so as to have each direction contain equal variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_whithened = pca.fit(pca.transform(z.centered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apoNN.src.utils as utils\n",
    "utils.dump(pca,\"pca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.load(\"pca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=8\n",
    "np.dot(w.val[0:1]/np.linalg.norm(w.val[0:1]),w.val[idx:idx+1].T/np.linalg.norm(w.val[idx:idx+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.val[0:1]/np.linalg.norm(w.val[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pca = np.array([ -0.43056273, -0.6169323 ,  0.46758795,  0.42295858, -0.2486121 ,\n",
    "        0.39564928, -0.0904031 ,  0.65999484,  0.91201407,  0.8673172 ,\n",
    "       -0.13539782,  0.54779094,  0.8583661 ,  0.14308473,  0.0373497 ,\n",
    "       -0.48964548, -0.0763603 , -0.23988001, -1.6411978 , -0.6619012 ,\n",
    "       -0.6396435 , -0.44550008,  0.39345187, -0.6311279 ,  0.97478664])\n",
    "idx=9\n",
    "np.dot(test_pca,w.val[idx:idx+1].T/np.linalg.norm(w.val[idx:idx+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(w.val[0:1]/np.linalg.norm(w.val[0:1]),w.val[4:5].T/np.linalg.norm(w.val[4:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(w.val[0:1],w.val[3:4].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure reconstruction\n",
    "\n",
    "As a safeguard we measure the reconstruction of those stars within our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.L1Loss()\n",
    "for idx in range(100):\n",
    "    x = pickled_dataset[idx][0].to(device).unsqueeze(0)\n",
    "    x_pred,z = autoencoder(x)\n",
    "    print(loss(x_pred,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_pred.detach().cpu().numpy()[0])\n",
    "plt.plot(pickled_dataset[idx][0])\n",
    "plt.xlim(5000,5500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taggenv",
   "language": "python",
   "name": "taggenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
