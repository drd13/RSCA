{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**notebook objective**:\n",
    "* Experiment with how to exclude zero'd out features in APOGEE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apogee.tools.read as apread\n",
    "import matplotlib.pyplot as plt\n",
    "import apogee.tools.path as apogee_path\n",
    "from apogee.tools import bitmask\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchcontrib.optim import SWA\n",
    "\n",
    "from apoNN.src.datasets import ApogeeDataset\n",
    "\n",
    "from tagging.src.networks import ConditioningAutoencoder,Embedding_Decoder,Feedforward,ParallelDecoder,Autoencoder\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "apogee_path.change_dr(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/splinter/ddm/taggenv/lib64/python3.6/site-packages/apogee/tools/read.py:303: RuntimeWarning: Extinction-corrected J,H,K not added because esutil is not installed\n",
      "  warnings.warn(\"Extinction-corrected J,H,K not added because esutil is not installed\",RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "allStar= apread.allStar(rmcommissioning=True,main=False,ak=True, akvers='targ',adddist=False)\n",
    "\n",
    "upper_temp_cut = allStar[\"Teff\"]<5000\n",
    "lower_temp_cut = allStar[\"Teff\"]>4000\n",
    "lower_g_cut = allStar[\"logg\"]>1.5\n",
    "upper_g_cut = allStar[\"logg\"]<3\n",
    "snr_cut = allStar[\"SNR\"]>100\n",
    "snr_highcut = allStar[\"SNR\"]<500\n",
    "feh_outliercut = allStar[\"Fe_H\"]>-5\n",
    "o_outliercut = allStar[\"O_FE\"]>-5\n",
    "c_outliercut = allStar[\"C_FE\"]>-5\n",
    "na_outliercut = allStar[\"Na_FE\"]>-5\n",
    "mg_outliercut = allStar[\"Mg_FE\"]>-5\n",
    "si_outliercut = allStar[\"Si_FE\"]>-5\n",
    "\n",
    "\n",
    "\n",
    "combined_cut = lower_g_cut & upper_g_cut & lower_temp_cut & upper_temp_cut & snr_cut & snr_highcut & feh_outliercut & o_outliercut &  c_outliercut & na_outliercut & mg_outliercut & si_outliercut\n",
    "cut_allStar = allStar[combined_cut]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batch = 128\n",
    "n_z = 80\n",
    "n_bins = 8575\n",
    "lr = 0.0001\n",
    "n_datapoints = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = ApogeeDataset(cut_allStar[:n_datapoints],outputs = [\"apstar\",\"physical\",\"idx\"])\n",
    "#dataset = ApogeeDataset(cut_allStar[:n_datapoints],outputs = [\"aspcap\",\"physical\",\"idx\"])\n",
    "\n",
    "dataset = ApogeeDataset(cut_allStar[:n_datapoints],outputs = [\"aspcap\",\"mask\",\"physical\",\"idx\"])\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset = dataset,\n",
    "                                     batch_size = n_batch,\n",
    "                                     shuffle= False,\n",
    "                                     drop_last=True)\n",
    "plt.plot(dataset[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of the neural network\n",
    "\n",
    "New training of neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Feedforward([n_bins,2048,512,n_z],activation=nn.SELU()).to(device)\n",
    "decoder = Feedforward([n_z,512,2048,8192,n_bins],activation=nn.SELU()).to(device)\n",
    "\n",
    "autoencoder = Autoencoder(encoder,decoder,n_bins=n_bins).to(device)\n",
    "optimizer_autoencoder = torch.optim.Adam(autoencoder.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use swa in order to find better minima (or at leasat in theory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#autoencoder = torch.load(\"/share/splinter/ddm/taggingProject/apogeeFactory/outputs/pretrained/ae1\")\n",
    "optimizer_autoencoder = torch.optim.Adam(autoencoder.parameters(), lr=lr)\n",
    "opt_swa = SWA(optimizer_autoencoder, swa_start=10, swa_freq=5, swa_lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_loss_with_masking(loss):\n",
    "    def loss_with_masking(x_pred,x_true):\n",
    "        non_zero = x_true!=0\n",
    "        return loss(x_pred[non_zero],x_true[non_zero])\n",
    "    return loss_with_masking\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_loss = generate_loss_with_masking(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20000):\n",
    "    for j,(x,mask,u,idx) in enumerate(loader):\n",
    "        opt_swa.zero_grad()\n",
    "        x_pred,z = autoencoder(x.to(device))\n",
    "\n",
    "        err_pred = masked_loss(x_pred,x.to(device))\n",
    "\n",
    "        err_tot = err_pred\n",
    "        err_tot.backward()\n",
    "        opt_swa.step()\n",
    "        if j%100==0:\n",
    "            print(f\"err:{err_tot},err_pred:{err_pred}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_z(idx,dataset):\n",
    "    _,z = autoencoder(dataset[idx][0].to(device).unsqueeze(0))\n",
    "    return z\n",
    "\n",
    "def get_v(idx,dataset,feedforward):\n",
    "    _,z = autoencoder(dataset[idx][0].to(device).unsqueeze(0))\n",
    "    v = feedforward(z)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taggenv",
   "language": "python",
   "name": "taggenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
